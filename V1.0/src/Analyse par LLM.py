"""
Analyseur Groq Complet - TOUS les 317 avis
==========================================

Version optimisÃ©e pour analyser l'intÃ©gralitÃ© de votre dataset avec Groq.
"""

import json
import pandas as pd
import time
import os
from pathlib import Path
from typing import List, Dict
from collections import Counter

try:
    from groq import Groq
    GROQ_AVAILABLE = True
except ImportError:
    GROQ_AVAILABLE = False
    print("âŒ Module 'groq' non installÃ©. Installez avec: pip install groq")

class CompleteGroqAnalyzer:
    """Analyseur Groq pour dataset complet."""
    
    def __init__(self, api_key: str = None):
        """Initialise l'analyseur Groq."""
        
        if not GROQ_AVAILABLE:
            raise ImportError("Module groq requis: pip install groq")
        
        # RÃ©cupÃ©rer clÃ© API
        self.api_key = api_key or os.getenv('GROQ_API_KEY')
        
        if not self.api_key:
            raise ValueError(
                "ğŸ”‘ ClÃ© API Groq requise!\n"
                "ğŸ’¡ Obtenez une clÃ© gratuite: https://console.groq.com/keys\n"
                "ğŸ”§ Puis: export GROQ_API_KEY=your_key"
            )
        
        # Initialiser client Groq
        self.client = Groq(api_key=self.api_key)
        
        # Statistiques Ã©tendues
        self.stats = {
            'api_calls': 0,
            'tokens_used': 0,
            'total_cost': 0.0,  # Groq gratuit
            'avg_response_time': 0.0,
            'successful_analyses': 0,
            'failed_analyses': 0,
            'start_time': None,
            'end_time': None
        }
        
        # Topics dÃ©couverts
        self.discovered_topics = []
        self.topic_descriptions = {}
        
        print("âœ… Groq Complete Analyzer initialisÃ©!")
        print(f"ğŸ¯ Objectif: Analyser TOUS vos 317 avis")
        print(f"ğŸ†“ Quota: 14,400/jour (largement suffisant!)")
    
    def discover_topics_enhanced(self, df_clean: pd.DataFrame, 
                                text_column: str = 'texte_clean') -> Dict:
        """Phase 1 amÃ©liorÃ©e: Discovery sur Ã©chantillon STRATIFIÃ‰."""
        
        print(f"ğŸ” PHASE 1: DÃ©couverte topics STRATIFIÃ‰E")
        
        # Ã‰chantillon stratifiÃ© plus large et Ã©quilibrÃ©
        discovery_samples = []
        
        print(f"ğŸ“Š Ã‰chantillonnage stratifiÃ© par note:")
        for note in sorted(df_clean['note'].unique()):
            note_reviews = df_clean[df_clean['note'] == note][text_column].tolist()
            
            # Prendre plus d'Ã©chantillons pour meilleure reprÃ©sentativitÃ©
            if note in [1, 2]:  # Notes nÃ©gatives
                sample_size = min(15, len(note_reviews))  # Max 15 par note nÃ©gative
            elif note == 3:      # Note neutre
                sample_size = min(10, len(note_reviews))  # 10 pour neutre
            else:                # Notes positives (4-5)
                sample_size = min(20, len(note_reviews))  # Plus pour positives
            
            selected = note_reviews[:sample_size]
            discovery_samples.extend(selected)
            print(f"   {note}â­: {len(selected)} avis sÃ©lectionnÃ©s")
        
        print(f"ğŸ“ˆ Total Ã©chantillon discovery: {len(discovery_samples)} avis")
        
        # Grouper en chunks pour Ã©viter prompts trop longs
        chunk_size = 25  # 25 avis par chunk
        all_topics = []
        
        for i in range(0, len(discovery_samples), chunk_size):
            chunk = discovery_samples[i:i+chunk_size]
            chunk_topics = self._discover_topics_chunk(chunk, chunk_num=i//chunk_size + 1)
            
            if chunk_topics:
                all_topics.extend(chunk_topics.get('discovered_topics', []))
        
        # Consolidation des topics dÃ©couverts
        topic_counter = Counter(all_topics)
        final_topics = [topic for topic, count in topic_counter.most_common(12)]  # Top 12 topics
        
        # GÃ©nÃ©rer descriptions consolidÃ©es
        consolidated_result = self._consolidate_discovered_topics(final_topics)
        
        self.discovered_topics = consolidated_result['discovered_topics']
        self.topic_descriptions = consolidated_result['topic_descriptions']
        
        print(f"ğŸ¯ {len(self.discovered_topics)} topics finaux consolidÃ©s:")
        for i, topic in enumerate(self.discovered_topics, 1):
            desc = self.topic_descriptions.get(topic, 'Pas de description')
            print(f"   {i}. {topic}")
            print(f"      ğŸ“ {desc}")
        
        return consolidated_result
    
    def _discover_topics_chunk(self, chunk_reviews: List[str], chunk_num: int) -> Dict:
        """DÃ©couvre topics sur un chunk d'avis."""
        
        print(f"   ğŸ” Analyse chunk {chunk_num} ({len(chunk_reviews)} avis)...", end=' ')
        
        sample_text = "\n".join([
            f"Avis {i+1}: {review[:150]}" for i, review in enumerate(chunk_reviews)
        ])
        
        prompt = f"""Tu es expert en analyse de feedback clients. Identifie les THÃˆMES RÃ‰CURRENTS dans ces avis sur une montre connectÃ©e.

IMPORTANT: 
- Base-toi UNIQUEMENT sur ce qui est mentionnÃ©
- Identifie AUTANT les aspects positifs que nÃ©gatifs
- Sois prÃ©cis et concret dans les thÃ¨mes

AVIS CLIENTS:
{sample_text}

RÃ©ponds UNIQUEMENT avec ce JSON:
{{
    "discovered_topics": ["theme1", "theme2", "theme3", "theme4", "theme5"],
    "confidence": 0.85
}}

JSON:"""

        try:
            start_time = time.time()
            
            response = self.client.chat.completions.create(
                model="llama-3.1-8b-instant",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                max_tokens=400
            )
            
            response_time = time.time() - start_time
            result_text = response.choices[0].message.content.strip()
            
            self.stats['api_calls'] += 1
            self.stats['avg_response_time'] = (
                (self.stats['avg_response_time'] * (self.stats['api_calls'] - 1) + response_time) 
                / self.stats['api_calls']
            )
            
            # Parser JSON
            try:
                result = json.loads(result_text)
                print(f"âœ… {len(result.get('discovered_topics', []))} topics")
                return result
            except json.JSONDecodeError:
                print("âš ï¸ JSON error")
                return {"discovered_topics": [], "confidence": 0.0}
                
        except Exception as e:
            print(f"âŒ {e}")
            return {"discovered_topics": [], "confidence": 0.0}
    
    def _consolidate_discovered_topics(self, all_topics: List[str]) -> Dict:
        """Consolide et dÃ©crit les topics dÃ©couverts."""
        
        print(f"ğŸ”„ Consolidation des topics...")
        
        # Descriptions gÃ©nÃ©riques basÃ©es sur patterns frÃ©quents
        topic_descriptions = {}
        
        for topic in all_topics:
            topic_lower = topic.lower()
            
            # Mapping intelligent des descriptions
            if any(word in topic_lower for word in ['batterie', 'autonomie', 'charge']):
                topic_descriptions[topic] = "DurÃ©e de vie de la batterie, temps de charge et autonomie"
            elif any(word in topic_lower for word in ['bluetooth', 'connexion', 'connectiv']):
                topic_descriptions[topic] = "ConnectivitÃ© Bluetooth, appairage et stabilitÃ© de connexion"
            elif any(word in topic_lower for word in ['app', 'application', 'logiciel']):
                topic_descriptions[topic] = "Application mobile, interface utilisateur et synchronisation"
            elif any(word in topic_lower for word in ['qualitÃ©', 'construction', 'matÃ©riau']):
                topic_descriptions[topic] = "QualitÃ© de construction, matÃ©riaux et finition"
            elif any(word in topic_lower for word in ['design', 'esthÃ©tique', 'apparence']):
                topic_descriptions[topic] = "Design, esthÃ©tique et apparence du produit"
            elif any(word in topic_lower for word in ['prix', 'rapport', 'coÃ»t']):
                topic_descriptions[topic] = "Prix, rapport qualitÃ©-prix et valeur perÃ§ue"
            elif any(word in topic_lower for word in ['durabilitÃ©', 'robustesse', 'soliditÃ©']):
                topic_descriptions[topic] = "DurabilitÃ©, robustesse et longÃ©vitÃ© du produit"
            elif any(word in topic_lower for word in ['fonctionnalitÃ©', 'fonction', 'feature']):
                topic_descriptions[topic] = "FonctionnalitÃ©s disponibles et utilitÃ©"
            elif any(word in topic_lower for word in ['confort', 'ergonomie', 'port']):
                topic_descriptions[topic] = "Confort de port et ergonomie"
            elif any(word in topic_lower for word in ['prÃ©cision', 'mesure', 'fiabilitÃ©']):
                topic_descriptions[topic] = "PrÃ©cision des mesures et fiabilitÃ© des donnÃ©es"
            else:
                topic_descriptions[topic] = f"Aspect important identifiÃ©: {topic}"
        
        return {
            'discovered_topics': all_topics,
            'topic_descriptions': topic_descriptions,
            'consolidation_method': 'frequency_based'
        }
    
    def analyze_complete_dataset(self, df_clean: pd.DataFrame,
                               text_column: str = 'texte_clean',
                               batch_size: int = 50) -> List[Dict]:
        """Phase 2: Analyse COMPLÃˆTE de tous les avis avec batch processing."""
        
        print(f"\nğŸ¤– PHASE 2: Analyse complÃ¨te de TOUS les avis")
        print(f"ğŸ“Š Dataset: {len(df_clean)} avis Ã  analyser")
        print(f"ğŸ“¦ Batch size: {batch_size} avis par batch")
        
        if not self.discovered_topics:
            raise ValueError("âŒ Phase 1 (discovery) doit Ãªtre exÃ©cutÃ©e d'abord!")
        
        self.stats['start_time'] = time.time()
        all_results = []
        
        # Traitement par batches pour optimiser
        total_batches = (len(df_clean) + batch_size - 1) // batch_size
        
        for batch_num in range(total_batches):
            start_idx = batch_num * batch_size
            end_idx = min(start_idx + batch_size, len(df_clean))
            batch_df = df_clean.iloc[start_idx:end_idx]
            
            print(f"\nğŸ“¦ BATCH {batch_num + 1}/{total_batches}")
            print(f"   ğŸ“„ Avis {start_idx + 1} Ã  {end_idx}")
            
            # Analyser le batch
            batch_results = self._analyze_batch(batch_df, text_column, batch_num + 1)
            all_results.extend(batch_results)
            
            # Pause entre batches pour Ã©viter rate limiting
            if batch_num < total_batches - 1:  # Pas de pause aprÃ¨s le dernier batch
                print(f"   â¸ï¸ Pause 2s...")
                time.sleep(2)
            
            # Stats intermÃ©diaires
            success_rate = (len(all_results) / end_idx) * 100
            print(f"   ğŸ“Š SuccÃ¨s: {len(all_results)}/{end_idx} ({success_rate:.1f}%)")
        
        self.stats['end_time'] = time.time()
        self.stats['successful_analyses'] = len(all_results)
        self.stats['failed_analyses'] = len(df_clean) - len(all_results)
        
        total_time = self.stats['end_time'] - self.stats['start_time']
        print(f"\nâœ… ANALYSE COMPLÃˆTE TERMINÃ‰E!")
        print(f"â±ï¸ Temps total: {total_time:.1f}s ({total_time/60:.1f} min)")
        print(f"ğŸ“Š RÃ©sultats: {len(all_results)}/{len(df_clean)} avis analysÃ©s")
        print(f"ğŸš€ Performance: {len(all_results)/total_time:.1f} avis/seconde")
        
        return all_results
    
    def _analyze_batch(self, batch_df: pd.DataFrame, 
                      text_column: str, batch_num: int) -> List[Dict]:
        """Analyse un batch d'avis."""
        
        batch_results = []
        
        for idx, row in batch_df.iterrows():
            try:
                review_text = str(row[text_column])
                if len(review_text.strip()) < 10:  # Skip avis trop courts
                    continue
                
                print(f"     ğŸ”„ Avis {len(batch_results) + 1}/{len(batch_df)}...", end=' ')
                
                # Analyse Groq de l'avis individuel
                analysis_result = self._analyze_single_review_optimized(review_text)
                
                if analysis_result:
                    # Combiner donnÃ©es originales + analyse Groq
                    full_result = {
                        'id': row.get('id', idx),
                        'note_originale': row['note'],
                        'auteur': row.get('auteur', 'Anonyme'),
                        'date': row.get('date', ''),
                        'avis_texte': review_text,
                        'avis_length': len(review_text),
                        **analysis_result  # RÃ©sultats Groq
                    }
                    
                    batch_results.append(full_result)
                    print(f"âœ… {analysis_result.get('sentiment_global', 'OK')}")
                    
                    self.stats['successful_analyses'] += 1
                else:
                    print("âŒ Ã‰chec")
                    self.stats['failed_analyses'] += 1
                
                # Micro-pause pour Ã©viter rate limiting
                time.sleep(0.1)
                
            except Exception as e:
                print(f"âŒ Erreur: {str(e)[:50]}...")
                self.stats['failed_analyses'] += 1
                continue
        
        return batch_results
    
    def _analyze_single_review_optimized(self, review_text: str) -> Dict:
        """Analyse optimisÃ©e d'un avis individuel."""
        
        topics_str = ", ".join(self.discovered_topics[:8])  # Limiter Ã  8 topics pour prompt
        
        prompt = f"""Analyse cet avis client sur une montre connectÃ©e.

ASPECTS IMPORTANTS IDENTIFIÃ‰S: {topics_str}

AVIS: "{review_text[:500]}"

Analyse le sentiment global et identifie quels aspects sont mentionnÃ©s.

RÃ©ponds UNIQUEMENT avec ce JSON:
{{
    "sentiment_global": "positif|neutre|nÃ©gatif",
    "score_sentiment": 0.75,
    "confiance": 0.85,
    "aspects_mentionnÃ©s": ["aspect1", "aspect2"],
    "sentiment_par_aspect": {{
        "aspect1": "positif|neutre|nÃ©gatif"
    }},
    "points_clÃ©s": ["Point principal 1", "Point principal 2"],
    "recommandation": "Action recommandÃ©e"
}}

JSON:"""

        try:
            start_time = time.time()
            
            response = self.client.chat.completions.create(
                model="llama-3.1-8b-instant",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                max_tokens=500
            )
            
            response_time = time.time() - start_time
            result_text = response.choices[0].message.content.strip()
            
            # Stats
            self.stats['api_calls'] += 1
            self.stats['avg_response_time'] = (
                (self.stats['avg_response_time'] * (self.stats['api_calls'] - 1) + response_time) 
                / self.stats['api_calls']
            )
            
            # Parser JSON
            try:
                return json.loads(result_text)
            except json.JSONDecodeError:
                # Fallback basique si parsing Ã©choue
                return {
                    "sentiment_global": "neutre",
                    "score_sentiment": 0.5,
                    "confiance": 0.3,
                    "aspects_mentionnÃ©s": [],
                    "sentiment_par_aspect": {},
                    "points_clÃ©s": ["Analyse partiellement Ã©chouÃ©e"],
                    "recommandation": "Revoir le prompt",
                    "parsing_error": True
                }
                
        except Exception as e:
            return None

def run_complete_analysis():
    """Pipeline complet d'analyse sur TOUS les avis."""
    
    print("ğŸš€ ANALYSE COMPLÃˆTE GROQ - TOUS LES AVIS")
    print("=" * 60)
    print("ğŸ¯ Objectif: Analyser TOUS vos 317 avis avec Groq")
    print("ğŸ†“ CoÃ»t: 0â‚¬ (Groq gratuit)")
    print("â±ï¸ DurÃ©e estimÃ©e: 5-10 minutes")
    print()
    
    # Configuration clÃ© API
    if not os.getenv('GROQ_API_KEY'):
        print("ğŸ”‘ Configuration clÃ© Groq:")
        api_key = input("Entrez votre clÃ© API Groq: ").strip()
        if not api_key:
            print("âŒ ClÃ© API requise")
            return
        os.environ['GROQ_API_KEY'] = api_key
    
    # Charger donnÃ©es
    data_file = "V1.0/data/processed/avis_amazon_clean_complete.json"
    if not Path(data_file).exists():
        print(f"âŒ Fichier non trouvÃ©: {data_file}")
        return
    
    df = pd.read_json(data_file)
    text_col = 'texte_clean' if 'texte_clean' in df.columns else 'texte'
    df_clean = df[df[text_col].notna()]
    df_clean = df_clean[df_clean[text_col].str.len() > 10]
    
    print(f"ğŸ“Š Dataset chargÃ©: {len(df_clean)} avis analysables")
    
    # Distribution par note
    note_dist = df_clean['note'].value_counts().sort_index()
    print(f"ğŸ“ˆ Distribution:")
    for note, count in note_dist.items():
        print(f"   {note}â­: {count} avis ({count/len(df_clean)*100:.1f}%)")
    
    # Confirmation utilisateur
    print(f"\nâš ï¸ ATTENTION:")
    print(f"   â€¢ Analyser {len(df_clean)} avis = ~{len(df_clean) + 20} appels API")
    print(f"   â€¢ Temps estimÃ©: {(len(df_clean) * 0.5)/60:.1f}-{(len(df_clean) * 1)/60:.1f} minutes")
    
    confirm = input("\nğŸš€ Lancer l'analyse complÃ¨te ? (y/N): ").strip().lower()
    if confirm != 'y':
        print("âŒ Analyse annulÃ©e")
        return
    
    try:
        # Initialiser analyseur
        analyzer = CompleteGroqAnalyzer()
        
        # PHASE 1: Discovery amÃ©liorÃ©e
        print(f"\nğŸ” PHASE 1: DISCOVERY STRATIFIÃ‰E")
        discovery_result = analyzer.discover_topics_enhanced(df_clean, text_col)
        
        # PHASE 2: Analyse complÃ¨te
        print(f"\nğŸ¤– PHASE 2: ANALYSE COMPLÃˆTE")
        all_results = analyzer.analyze_complete_dataset(df_clean, text_col, batch_size=25)
        
        # PHASE 3: AgrÃ©gation et sauvegarde
        print(f"\nğŸ“Š PHASE 3: AGRÃ‰GATION RÃ‰SULTATS")
        
        # Stats finales
        total_time = analyzer.stats['end_time'] - analyzer.stats['start_time']
        print(f"ğŸ“ˆ STATISTIQUES FINALES:")
        print(f"   ğŸ“ Appels API: {analyzer.stats['api_calls']}")
        print(f"   â±ï¸ Temps total: {total_time:.1f}s")
        print(f"   ğŸ¯ SuccÃ¨s: {len(all_results)}/{len(df_clean)} ({len(all_results)/len(df_clean)*100:.1f}%)")
        print(f"   ğŸš€ Performance: {analyzer.stats['avg_response_time']:.2f}s/avis moyenne")
        
        # Sauvegarde rÃ©sultats complets
        output_file = "V1.0/data/processed/groq_complete_analysis_results.json"
        complete_results = {
            'metadata': {
                'analysis_date': '2025-01-09',
                'total_reviews_analyzed': len(all_results),
                'total_reviews_in_dataset': len(df_clean),
                'success_rate': len(all_results) / len(df_clean),
                'analysis_duration_seconds': total_time,
                'api_calls_used': analyzer.stats['api_calls']
            },
            'discovery_phase': discovery_result,
            'analysis_results': all_results,
            'performance_stats': analyzer.stats
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(complete_results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ RÃ©sultats sauvÃ©s: {output_file}")
        
        # AperÃ§u rÃ©sultats
        print(f"\nğŸ“Š APERÃ‡U RÃ‰SULTATS:")
        sentiments = [r.get('sentiment_global', 'inconnu') for r in all_results]
        sentiment_dist = Counter(sentiments)
        
        for sentiment, count in sentiment_dist.items():
            pct = (count / len(all_results)) * 100
            emoji = "ğŸ˜Š" if sentiment == "positif" else "ğŸ˜" if sentiment == "neutre" else "ğŸ˜"
            print(f"   {emoji} {sentiment.upper()}: {count} avis ({pct:.1f}%)")
        
        # Aspects les plus mentionnÃ©s
        all_aspects = []
        for r in all_results:
            aspects = r.get('aspects_mentionnÃ©s', [])
            all_aspects.extend(aspects)
        
        if all_aspects:
            aspect_counts = Counter(all_aspects)
            print(f"\nğŸ”¥ TOP ASPECTS MENTIONNÃ‰S:")
            for aspect, count in aspect_counts.most_common(8):
                pct = (count / len(all_results)) * 100
                print(f"   ğŸ“Œ {aspect}: {count} mentions ({pct:.1f}%)")
        
        print(f"\nğŸ‰ ANALYSE COMPLÃˆTE TERMINÃ‰E AVEC SUCCÃˆS!")
        print(f"ğŸ”¥ Vous disposez maintenant d'une analyse LLM de TOUS vos avis!")
        
    except Exception as e:
        print(f"\nâŒ Erreur pendant l'analyse: {e}")
        print("ğŸ’¡ VÃ©rifiez votre clÃ© API et connexion internet")

if __name__ == "__main__":
    run_complete_analysis()